{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Extract Concentration g/L</th>\n",
       "      <th>KI Concentration M</th>\n",
       "      <th>Immersion Time H</th>\n",
       "      <th>Temperature Deg C</th>\n",
       "      <th>IE%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>81.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>71.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>63.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>83.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>72.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1.25</td>\n",
       "      <td>0.055</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>90.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1.25</td>\n",
       "      <td>0.055</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>88.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1.25</td>\n",
       "      <td>0.055</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>90.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1.25</td>\n",
       "      <td>0.055</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>87.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1.25</td>\n",
       "      <td>0.055</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>88.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Extract Concentration g/L  KI Concentration M  Immersion Time H  \\\n",
       "0                        0.50               0.000                 2   \n",
       "1                        0.50               0.000                 2   \n",
       "2                        0.50               0.000                 2   \n",
       "3                        0.50               0.000                 5   \n",
       "4                        0.50               0.000                 5   \n",
       "..                        ...                 ...               ...   \n",
       "74                       1.25               0.055                 5   \n",
       "75                       1.25               0.055                 5   \n",
       "76                       1.25               0.055                 2   \n",
       "77                       1.25               0.055                 8   \n",
       "78                       1.25               0.055                 5   \n",
       "\n",
       "    Temperature Deg C    IE%  \n",
       "0                  30  81.63  \n",
       "1                  40  71.24  \n",
       "2                  50  63.21  \n",
       "3                  30  83.96  \n",
       "4                  40  72.91  \n",
       "..                ...    ...  \n",
       "74                 30  90.59  \n",
       "75                 50  88.83  \n",
       "76                 40  90.90  \n",
       "77                 40  87.32  \n",
       "78                 40  88.36  \n",
       "\n",
       "[79 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file specifying the delimiter as a semicolon\n",
    "df = pd.read_csv(\"Projet 2.csv\", sep=\";\", decimal=\",\")\n",
    "\n",
    "\n",
    "# Display the first few rows to confirm it loaded correctly\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before imputation:\n",
      "Extract Concentration g/L    0\n",
      "KI Concentration M           0\n",
      "Immersion Time H             0\n",
      "Temperature Deg C            0\n",
      "IE%                          0\n",
      "dtype: int64\n",
      "Preprocessed (scaled) data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Extract Concentration g/L</th>\n",
       "      <th>KI Concentration M</th>\n",
       "      <th>Immersion Time H</th>\n",
       "      <th>Temperature Deg C</th>\n",
       "      <th>IE%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.414019</td>\n",
       "      <td>-0.732985</td>\n",
       "      <td>-1.20953</td>\n",
       "      <td>-1.20953</td>\n",
       "      <td>-0.205788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.414019</td>\n",
       "      <td>-0.732985</td>\n",
       "      <td>-1.20953</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-1.544562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.414019</td>\n",
       "      <td>-0.732985</td>\n",
       "      <td>-1.20953</td>\n",
       "      <td>1.20953</td>\n",
       "      <td>-2.378236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.414019</td>\n",
       "      <td>-0.732985</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-1.20953</td>\n",
       "      <td>0.094437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.414019</td>\n",
       "      <td>-0.732985</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-1.329379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.547966</td>\n",
       "      <td>2.693395</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-1.20953</td>\n",
       "      <td>0.948727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.547966</td>\n",
       "      <td>2.693395</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.20953</td>\n",
       "      <td>0.721947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.547966</td>\n",
       "      <td>2.693395</td>\n",
       "      <td>-1.20953</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.988671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.547966</td>\n",
       "      <td>2.693395</td>\n",
       "      <td>1.20953</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.527380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.547966</td>\n",
       "      <td>2.693395</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.661387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Extract Concentration g/L  KI Concentration M  Immersion Time H  \\\n",
       "0                   -0.414019           -0.732985          -1.20953   \n",
       "1                   -0.414019           -0.732985          -1.20953   \n",
       "2                   -0.414019           -0.732985          -1.20953   \n",
       "3                   -0.414019           -0.732985           0.00000   \n",
       "4                   -0.414019           -0.732985           0.00000   \n",
       "..                        ...                 ...               ...   \n",
       "74                   0.547966            2.693395           0.00000   \n",
       "75                   0.547966            2.693395           0.00000   \n",
       "76                   0.547966            2.693395          -1.20953   \n",
       "77                   0.547966            2.693395           1.20953   \n",
       "78                   0.547966            2.693395           0.00000   \n",
       "\n",
       "    Temperature Deg C       IE%  \n",
       "0            -1.20953 -0.205788  \n",
       "1             0.00000 -1.544562  \n",
       "2             1.20953 -2.378236  \n",
       "3            -1.20953  0.094437  \n",
       "4             0.00000 -1.329379  \n",
       "..                ...       ...  \n",
       "74           -1.20953  0.948727  \n",
       "75            1.20953  0.721947  \n",
       "76            0.00000  0.988671  \n",
       "77            0.00000  0.527380  \n",
       "78            0.00000  0.661387  \n",
       "\n",
       "[79 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Read the CSV file:\n",
    "#    - The file is delimited by semicolons.\n",
    "#    - The 'decimal' parameter tells pandas to interpret commas as decimal points.\n",
    "df = pd.read_csv(\"Projet 2.csv\", sep=\";\", decimal=\",\")\n",
    "\n",
    "# 2. Check for missing values:\n",
    "print(\"Missing values before imputation:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# 3. Impute missing values using the median strategy.\n",
    "#    This is especially helpful when the number of data points is small.\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# 4. Handle outliers using the IQR method:\n",
    "def cap_outliers(col):\n",
    "    # Calculate the 25th (Q1) and 75th (Q3) percentiles.\n",
    "    Q1 = col.quantile(0.25)\n",
    "    Q3 = col.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    # Define lower and upper bounds (1.5 times the IQR)\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    # Cap the values outside the bounds\n",
    "    return col.clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "# Apply the outlier capping function to all numerical columns.\n",
    "# (If some columns are non-numeric, you may need to select only the numeric ones.)\n",
    "df_imputed = df_imputed.apply(cap_outliers)\n",
    "\n",
    "# 5. Feature scaling: Standardize the features.\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_imputed), columns=df_imputed.columns)\n",
    "\n",
    "# Optional: Display the first few rows of the preprocessed data.\n",
    "print(\"Preprocessed (scaled) data:\")\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, RidgeCV, LassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Assume df_scaled is your preprocessed DataFrame.\n",
    "# For example, it might look like:\n",
    "#    feature1   feature2   feature3   ...   IE%\n",
    "# 0  -0.414019  -0.732985  -1.20953   ...   0.205788\n",
    "# 1  -0.414019  -0.732985   0.00000   ...  -1.544562\n",
    "# ...\n",
    "\n",
    "# 1. Separate the predictors (X) from the target variable (y), which is in the column \"IE%\"\n",
    "X = df_scaled.drop('IE%', axis=1)\n",
    "y = df_scaled['IE%']\n",
    "\n",
    "# 2. Split the data into training and testing sets (e.g., 80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression (fixed alpha = 1.0):\n",
      "Train MSE: 0.6423566080748396\n",
      "Test MSE: 0.3203918899431294\n",
      "Test R²: 0.20951697741065622\n",
      "\n",
      "Ridge Regression with Cross-Validation:\n",
      "Best alpha selected: 20.09233002565046\n",
      "Test MSE (RidgeCV): 0.2998344708863525\n",
      "Test R² (RidgeCV): 0.26023702140278493\n",
      "\n",
      "Lasso Regression with Cross-Validation:\n",
      "Best alpha selected: 0.057820399419265346\n",
      "Test MSE (LassoCV): 0.27411224835167847\n",
      "Test R² (LassoCV): 0.3236998644246044\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------\n",
    "# 3. Regularized Linear Model Without Cross-Validation\n",
    "#    Using Ridge regression with a fixed alpha value.\n",
    "# ------------------------------------------\n",
    "alpha_value = 1 # 1.0  # Choose an alpha value\n",
    "ridge_model = Ridge(alpha=alpha_value)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on train and test sets\n",
    "y_train_pred = ridge_model.predict(X_train)\n",
    "y_test_pred = ridge_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Ridge Regression (fixed alpha = 1.0):\")\n",
    "print(\"Train MSE:\", mean_squared_error(y_train, y_train_pred))\n",
    "print(\"Test MSE:\", mean_squared_error(y_test, y_test_pred))\n",
    "print(\"Test R²:\", r2_score(y_test, y_test_pred))\n",
    "\n",
    "# ------------------------------------------\n",
    "# 4. Regularized Model with Cross-Validation for Hyperparameter Tuning\n",
    "#    Using RidgeCV to select the best alpha from a candidate set.\n",
    "# ------------------------------------------\n",
    "# Define a candidate range for alpha (regularization strength)\n",
    "alphas = np.logspace(-3, 3, 100)  # From 0.001 to 1000\n",
    "ridge_cv = RidgeCV(alphas=alphas, cv=5)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nRidge Regression with Cross-Validation:\")\n",
    "print(\"Best alpha selected:\", ridge_cv.alpha_)\n",
    "\n",
    "# Evaluate the tuned RidgeCV model on the test set.\n",
    "y_test_pred_cv = ridge_cv.predict(X_test)\n",
    "print(\"Test MSE (RidgeCV):\", mean_squared_error(y_test, y_test_pred_cv))\n",
    "print(\"Test R² (RidgeCV):\", r2_score(y_test, y_test_pred_cv))\n",
    "\n",
    "# ------------------------------------------\n",
    "# 5. Using LassoCV for Lasso Regression (another regularization method)\n",
    "# ------------------------------------------\n",
    "lasso_cv = LassoCV(alphas=None, cv=5, random_state=42)  # Let LassoCV determine the best alpha\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nLasso Regression with Cross-Validation:\")\n",
    "print(\"Best alpha selected:\", lasso_cv.alpha_)\n",
    "\n",
    "# Evaluate the Lasso model on the test set.\n",
    "y_test_pred_lasso = lasso_cv.predict(X_test)\n",
    "print(\"Test MSE (LassoCV):\", mean_squared_error(y_test, y_test_pred_lasso))\n",
    "print(\"Test R² (LassoCV):\", r2_score(y_test, y_test_pred_lasso))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Linear Models with Regularization  i ned to try with the Outlier Treatment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeCV with Polynomial Features:\n",
      "Best alpha selected: 0.6135907273413176\n",
      "Test MSE: 0.2126807080396137\n",
      "Test R²: 0.4752660906384444\n",
      "\n",
      "LassoCV with Polynomial Features:\n",
      "Best alpha selected: 0.01778902890119987\n",
      "Test MSE: 0.19373047874210853\n",
      "Test R²: 0.522020815099516\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Assume df_scaled is your preprocessed DataFrame with the target column \"IE%\"\n",
    "# For example, your data may look like:\n",
    "#    feature1   feature2   feature3   ...   IE%\n",
    "# 0  -0.414019  -0.732985  -1.20953   ...   0.205788\n",
    "# 1  -0.414019  -0.732985   0.00000   ...  -1.544562\n",
    "# ...\n",
    "\n",
    "# Separate predictors (X) and target (y)\n",
    "X = df_scaled.drop('IE%', axis=1)\n",
    "y = df_scaled['IE%']\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ------------------------------------------\n",
    "# Create a pipeline for Ridge Regression with polynomial features\n",
    "# ------------------------------------------\n",
    "ridge_pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),  # Generate polynomial features (squared & interactions)\n",
    "    ('scaler', StandardScaler()),                                # Scale the features\n",
    "    ('ridgecv', RidgeCV(alphas=np.logspace(-3, 3, 100), cv=5))     # Tune alpha via 5-fold CV\n",
    "])\n",
    "\n",
    "ridge_pipeline.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge_pipeline.predict(X_test)\n",
    "\n",
    "print(\"RidgeCV with Polynomial Features:\")\n",
    "print(\"Best alpha selected:\", ridge_pipeline.named_steps['ridgecv'].alpha_)\n",
    "print(\"Test MSE:\", mean_squared_error(y_test, y_pred_ridge))\n",
    "print(\"Test R²:\", r2_score(y_test, y_pred_ridge))\n",
    "\n",
    "# ------------------------------------------\n",
    "# Create a pipeline for Lasso Regression with polynomial features\n",
    "# ------------------------------------------\n",
    "lasso_pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lassocv', LassoCV(alphas=None, cv=5, random_state=42))       # LassoCV will choose its own alpha range\n",
    "])\n",
    "\n",
    "lasso_pipeline.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso_pipeline.predict(X_test)\n",
    "\n",
    "print(\"\\nLassoCV with Polynomial Features:\")\n",
    "print(\"Best alpha selected:\", lasso_pipeline.named_steps['lassocv'].alpha_)\n",
    "print(\"Test MSE:\", mean_squared_error(y_test, y_pred_lasso))\n",
    "print(\"Test R²:\", r2_score(y_test, y_pred_lasso))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total polynomial features: 14\n",
      "Option A: LassoCV selected 14 out of 14 features.\n",
      "\n",
      "Option A (Ridge on LassoCV-selected features):\n",
      "Test MSE: 0.24941847184827753\n",
      "Test R²: 0.38462528639149296\n",
      "\n",
      "Option B: Full polynomial feature set has 14 features. Selecting 7 features via RFE.\n",
      "RFE selected 7 features.\n",
      "\n",
      "Option B (Ridge on RFE-selected features):\n",
      "Test MSE: 0.23253784995743076\n",
      "Test R²: 0.4262737969634458\n",
      "\n",
      "Option C (LassoCV directly on full features):\n",
      "Best alpha selected: 0.01778902890119987\n",
      "Test MSE: 0.19373047874210853\n",
      "Test R²: 0.522020815099516\n",
      "\n",
      "Option D: RFE with LassoCV as base estimator selected 7 features.\n",
      "\n",
      "Option D (Ridge on RFE-selected features with LassoCV base):\n",
      "Test MSE: 0.23232688203398888\n",
      "Test R²: 0.42679430502568594\n",
      "\n",
      "Ensemble (average of Options A, B, and C):\n",
      "Test MSE: 0.20036763349300135\n",
      "Test R²: 0.5056453751662198\n",
      "\n",
      "ElasticNetCV on full polynomial features:\n",
      "Best alpha: 0.01873817422860384\n",
      "Best l1_ratio: 0.5\n",
      "Test MSE: 0.20122528577691487\n",
      "Test R²: 0.5035293429225854\n",
      "\n",
      "SVR (RBF kernel) with GridSearchCV:\n",
      "Best parameters: {'C': 100, 'gamma': 0.001}\n",
      "Test MSE: 0.26217393658682975\n",
      "Test R²: 0.35315452000332903\n",
      "\n",
      "Nested CV for Ridge Regression Pipeline:\n",
      "Average Nested CV Test MSE: 0.49028300493140664\n",
      "\n",
      "Summary of Model Performance:\n",
      "                               Option Selected Features Best Alpha  Test MSE  \\\n",
      "0       A (Ridge on LassoCV-selected)                14        NaN  0.249418   \n",
      "1           B (Ridge on RFE-selected)                 7        NaN  0.232538   \n",
      "2                C (LassoCV directly)                14   0.017789  0.193730   \n",
      "3  D (Ridge on RFE(LassoCV)-selected)                 7        NaN  0.232327   \n",
      "4            Ensemble (A+B+C average)                 -          -  0.200368   \n",
      "5                        ElasticNetCV                14   0.018738  0.201225   \n",
      "6                    SVR (RBF kernel)                14          -  0.262174   \n",
      "7          Nested CV (Ridge Pipeline)          Pipeline          -  0.490283   \n",
      "\n",
      "    Test R²  \n",
      "0  0.384625  \n",
      "1  0.426274  \n",
      "2  0.522021  \n",
      "3  0.426794  \n",
      "4  0.505645  \n",
      "5  0.503529  \n",
      "6  0.353155  \n",
      "7         -  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Ridge, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ------------------------------\n",
    "# PREPROCESSING: Polynomial Feature Generation & Scaling\n",
    "# ------------------------------\n",
    "# Assume df_scaled is your preprocessed DataFrame with target column \"IE%\"\n",
    "X = df_scaled.drop('IE%', axis=1)\n",
    "y = df_scaled['IE%']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Generate degree-2 polynomial features.\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# Scale the polynomial features.\n",
    "scaler = StandardScaler()\n",
    "X_train_poly_scaled = scaler.fit_transform(X_train_poly)\n",
    "X_test_poly_scaled = scaler.transform(X_test_poly)\n",
    "\n",
    "# Total number of polynomial features generated:\n",
    "n_poly_features = X_train_poly_scaled.shape[1]\n",
    "print(\"Total polynomial features:\", n_poly_features)\n",
    "\n",
    "# Dictionary to hold the performance results of each model option\n",
    "results = []\n",
    "\n",
    "# ------------------------------\n",
    "# OPTION A: LassoCV for Feature Selection, then Ridge\n",
    "# ------------------------------\n",
    "lasso_cv = LassoCV(alphas=None, cv=5, random_state=42)\n",
    "lasso_cv.fit(X_train_poly_scaled, y_train)\n",
    "\n",
    "# Identify non-zero coefficient indices.\n",
    "nonzero_indices = np.where(lasso_cv.coef_ != 0)[0]\n",
    "n_nonzero_A = len(nonzero_indices)\n",
    "print(\"Option A: LassoCV selected {} out of {} features.\".format(n_nonzero_A, n_poly_features))\n",
    "\n",
    "# Reduce datasets to selected features.\n",
    "X_train_A = X_train_poly_scaled[:, nonzero_indices]\n",
    "X_test_A  = X_test_poly_scaled[:, nonzero_indices]\n",
    "\n",
    "# Retrain Ridge on the LassoCV-selected features.\n",
    "ridge_A = Ridge(alpha=20.09)\n",
    "ridge_A.fit(X_train_A, y_train)\n",
    "y_pred_A = ridge_A.predict(X_test_A)\n",
    "\n",
    "mse_A = mean_squared_error(y_test, y_pred_A)\n",
    "r2_A = r2_score(y_test, y_pred_A)\n",
    "print(\"\\nOption A (Ridge on LassoCV-selected features):\")\n",
    "print(\"Test MSE:\", mse_A)\n",
    "print(\"Test R²:\", r2_A)\n",
    "\n",
    "results.append({\n",
    "    'Option': 'A (Ridge on LassoCV-selected)',\n",
    "    'Selected Features': n_nonzero_A,\n",
    "    'Best Alpha': np.nan,  # Not applicable since Ridge was fixed here.\n",
    "    'Test MSE': mse_A,\n",
    "    'Test R²': r2_A\n",
    "})\n",
    "\n",
    "# ------------------------------\n",
    "# OPTION B: RFE with Ridge estimator, then Ridge\n",
    "# ------------------------------\n",
    "ridge_estimator = Ridge(alpha=20.09)\n",
    "n_features_to_select_B = n_poly_features // 2  # Example: select half the features\n",
    "print(\"\\nOption B: Full polynomial feature set has {} features. Selecting {} features via RFE.\"\n",
    "      .format(n_poly_features, n_features_to_select_B))\n",
    "\n",
    "rfe = RFE(estimator=ridge_estimator, n_features_to_select=n_features_to_select_B, step=1)\n",
    "rfe.fit(X_train_poly_scaled, y_train)\n",
    "selected_features_B = rfe.support_\n",
    "n_selected_B = np.sum(selected_features_B)\n",
    "print(\"RFE selected {} features.\".format(n_selected_B))\n",
    "\n",
    "X_train_B = X_train_poly_scaled[:, selected_features_B]\n",
    "X_test_B  = X_test_poly_scaled[:, selected_features_B]\n",
    "\n",
    "ridge_B = Ridge(alpha=20.09)\n",
    "ridge_B.fit(X_train_B, y_train)\n",
    "y_pred_B = ridge_B.predict(X_test_B)\n",
    "\n",
    "mse_B = mean_squared_error(y_test, y_pred_B)\n",
    "r2_B = r2_score(y_test, y_pred_B)\n",
    "print(\"\\nOption B (Ridge on RFE-selected features):\")\n",
    "print(\"Test MSE:\", mse_B)\n",
    "print(\"Test R²:\", r2_B)\n",
    "\n",
    "results.append({\n",
    "    'Option': 'B (Ridge on RFE-selected)',\n",
    "    'Selected Features': n_selected_B,\n",
    "    'Best Alpha': np.nan,\n",
    "    'Test MSE': mse_B,\n",
    "    'Test R²': r2_B\n",
    "})\n",
    "\n",
    "# ------------------------------\n",
    "# OPTION C: Train LassoCV Directly on Full Polynomial Features\n",
    "# ------------------------------\n",
    "lasso_model = LassoCV(alphas=None, cv=5, random_state=42)\n",
    "lasso_model.fit(X_train_poly_scaled, y_train)\n",
    "y_pred_C = lasso_model.predict(X_test_poly_scaled)\n",
    "\n",
    "mse_C = mean_squared_error(y_test, y_pred_C)\n",
    "r2_C = r2_score(y_test, y_pred_C)\n",
    "print(\"\\nOption C (LassoCV directly on full features):\")\n",
    "print(\"Best alpha selected:\", lasso_model.alpha_)\n",
    "print(\"Test MSE:\", mse_C)\n",
    "print(\"Test R²:\", r2_C)\n",
    "\n",
    "results.append({\n",
    "    'Option': 'C (LassoCV directly)',\n",
    "    'Selected Features': n_poly_features,  # All features used.\n",
    "    'Best Alpha': lasso_model.alpha_,\n",
    "    'Test MSE': mse_C,\n",
    "    'Test R²': r2_C\n",
    "})\n",
    "\n",
    "# ------------------------------\n",
    "# OPTION D: RFE with LassoCV as Base Estimator, then Ridge\n",
    "# ------------------------------\n",
    "lasso_cv_for_rfe = LassoCV(alphas=None, cv=5, random_state=42)\n",
    "n_features_to_select_D = n_poly_features // 2  # For example, select half of the features\n",
    "rfe_D = RFE(estimator=lasso_cv_for_rfe, n_features_to_select=n_features_to_select_D, step=1)\n",
    "rfe_D.fit(X_train_poly_scaled, y_train)\n",
    "selected_features_D = rfe_D.support_\n",
    "n_selected_D = np.sum(selected_features_D)\n",
    "print(\"\\nOption D: RFE with LassoCV as base estimator selected {} features.\".format(n_selected_D))\n",
    "\n",
    "X_train_D = X_train_poly_scaled[:, selected_features_D]\n",
    "X_test_D  = X_test_poly_scaled[:, selected_features_D]\n",
    "\n",
    "ridge_D = Ridge(alpha=20.09)\n",
    "ridge_D.fit(X_train_D, y_train)\n",
    "y_pred_D = ridge_D.predict(X_test_D)\n",
    "\n",
    "mse_D = mean_squared_error(y_test, y_pred_D)\n",
    "r2_D = r2_score(y_test, y_pred_D)\n",
    "print(\"\\nOption D (Ridge on RFE-selected features with LassoCV base):\")\n",
    "print(\"Test MSE:\", mse_D)\n",
    "print(\"Test R²:\", r2_D)\n",
    "\n",
    "results.append({\n",
    "    'Option': 'D (Ridge on RFE(LassoCV)-selected)',\n",
    "    'Selected Features': n_selected_D,\n",
    "    'Best Alpha': np.nan,\n",
    "    'Test MSE': mse_D,\n",
    "    'Test R²': r2_D\n",
    "})\n",
    "\n",
    "# ------------------------------\n",
    "# ENSEMBLE / STACKING: Averaging Predictions from Options A, B, and C\n",
    "# ------------------------------\n",
    "ensemble_pred = (y_pred_A + y_pred_B + y_pred_C) / 3.0\n",
    "mse_ensemble = mean_squared_error(y_test, ensemble_pred)\n",
    "r2_ensemble = r2_score(y_test, ensemble_pred)\n",
    "print(\"\\nEnsemble (average of Options A, B, and C):\")\n",
    "print(\"Test MSE:\", mse_ensemble)\n",
    "print(\"Test R²:\", r2_ensemble)\n",
    "\n",
    "results.append({\n",
    "    'Option': 'Ensemble (A+B+C average)',\n",
    "    'Selected Features': '-',  # Not applicable for ensemble.\n",
    "    'Best Alpha': '-',        # Not applicable.\n",
    "    'Test MSE': mse_ensemble,\n",
    "    'Test R²': r2_ensemble\n",
    "})\n",
    "\n",
    "# ------------------------------\n",
    "# Additional Model: ElasticNetCV on full polynomial features\n",
    "# ------------------------------\n",
    "elastic_net_cv = ElasticNetCV(cv=5, random_state=42, alphas=np.logspace(-3, 3, 100))\n",
    "elastic_net_cv.fit(X_train_poly_scaled, y_train)\n",
    "y_pred_en = elastic_net_cv.predict(X_test_poly_scaled)\n",
    "mse_en = mean_squared_error(y_test, y_pred_en)\n",
    "r2_en = r2_score(y_test, y_pred_en)\n",
    "print(\"\\nElasticNetCV on full polynomial features:\")\n",
    "print(\"Best alpha:\", elastic_net_cv.alpha_)\n",
    "print(\"Best l1_ratio:\", elastic_net_cv.l1_ratio_)\n",
    "print(\"Test MSE:\", mse_en)\n",
    "print(\"Test R²:\", r2_en)\n",
    "\n",
    "results.append({\n",
    "    'Option': 'ElasticNetCV',\n",
    "    'Selected Features': n_poly_features,\n",
    "    'Best Alpha': elastic_net_cv.alpha_,\n",
    "    'Test MSE': mse_en,\n",
    "    'Test R²': r2_en\n",
    "})\n",
    "\n",
    "# ------------------------------\n",
    "# Additional Model: Support Vector Regression (SVR) with RBF kernel\n",
    "# ------------------------------\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "svr = SVR(kernel='rbf')\n",
    "grid_search = GridSearchCV(svr, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train_poly_scaled, y_train)\n",
    "best_svr = grid_search.best_estimator_\n",
    "y_pred_svr = best_svr.predict(X_test_poly_scaled)\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "print(\"\\nSVR (RBF kernel) with GridSearchCV:\")\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Test MSE:\", mse_svr)\n",
    "print(\"Test R²:\", r2_svr)\n",
    "\n",
    "results.append({\n",
    "    'Option': 'SVR (RBF kernel)',\n",
    "    'Selected Features': n_poly_features,\n",
    "    'Best Alpha': '-',  # Not applicable for SVR.\n",
    "    'Test MSE': mse_svr,\n",
    "    'Test R²': r2_svr\n",
    "})\n",
    "\n",
    "# ------------------------------\n",
    "# Nested Cross-Validation Example with Ridge Regression Pipeline\n",
    "# ------------------------------\n",
    "pipeline_ridge = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ridgecv', RidgeCV(alphas=np.logspace(-3, 3, 100), cv=5))\n",
    "])\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "nested_scores = cross_val_score(pipeline_ridge, X, y, cv=outer_cv, scoring='neg_mean_squared_error')\n",
    "avg_nested_mse = -np.mean(nested_scores)\n",
    "print(\"\\nNested CV for Ridge Regression Pipeline:\")\n",
    "print(\"Average Nested CV Test MSE:\", avg_nested_mse)\n",
    "\n",
    "results.append({\n",
    "    'Option': 'Nested CV (Ridge Pipeline)',\n",
    "    'Selected Features': 'Pipeline',\n",
    "    'Best Alpha': '-',  # Not directly applicable.\n",
    "    'Test MSE': avg_nested_mse,\n",
    "    'Test R²': '-'      # R² not computed in nested CV summary.\n",
    "})\n",
    "\n",
    "# ------------------------------\n",
    "# Display all results in a summary table.\n",
    "# ------------------------------\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSummary of Model Performance:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Option</th>\n",
       "      <th>Selected Features</th>\n",
       "      <th>Best Alpha</th>\n",
       "      <th>Test MSE</th>\n",
       "      <th>Test R²</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A (Ridge on LassoCV-selected)</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.249418</td>\n",
       "      <td>0.384625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B (Ridge on RFE-selected)</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.232538</td>\n",
       "      <td>0.426274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C (LassoCV directly)</td>\n",
       "      <td>14</td>\n",
       "      <td>0.017789</td>\n",
       "      <td>0.193730</td>\n",
       "      <td>0.522021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D (Ridge on RFE(LassoCV)-selected)</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.232327</td>\n",
       "      <td>0.426794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ensemble (A+B+C average)</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.200368</td>\n",
       "      <td>0.505645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ElasticNetCV</td>\n",
       "      <td>14</td>\n",
       "      <td>0.018738</td>\n",
       "      <td>0.201225</td>\n",
       "      <td>0.503529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SVR (RBF kernel)</td>\n",
       "      <td>14</td>\n",
       "      <td>-</td>\n",
       "      <td>0.262174</td>\n",
       "      <td>0.353155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nested CV (Ridge Pipeline)</td>\n",
       "      <td>Pipeline</td>\n",
       "      <td>-</td>\n",
       "      <td>0.490283</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Option Selected Features Best Alpha  Test MSE  \\\n",
       "0       A (Ridge on LassoCV-selected)                14        NaN  0.249418   \n",
       "1           B (Ridge on RFE-selected)                 7        NaN  0.232538   \n",
       "2                C (LassoCV directly)                14   0.017789  0.193730   \n",
       "3  D (Ridge on RFE(LassoCV)-selected)                 7        NaN  0.232327   \n",
       "4            Ensemble (A+B+C average)                 -          -  0.200368   \n",
       "5                        ElasticNetCV                14   0.018738  0.201225   \n",
       "6                    SVR (RBF kernel)                14          -  0.262174   \n",
       "7          Nested CV (Ridge Pipeline)          Pipeline          -  0.490283   \n",
       "\n",
       "    Test R²  \n",
       "0  0.384625  \n",
       "1  0.426274  \n",
       "2  0.522021  \n",
       "3  0.426794  \n",
       "4  0.505645  \n",
       "5  0.503529  \n",
       "6  0.353155  \n",
       "7         -  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of Best Model Results:\n",
      "                 Option  Selected Features  Best Alpha  Test MSE   Test R²\n",
      "2  C (LassoCV directly)                 14    0.017789   0.19373  0.522021\n",
      "2  C (LassoCV directly)                 14    0.017789   0.19373  0.522021\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'Test MSE' and 'Test R²' columns to numeric, coercing errors (i.e. '-' becomes NaN)\n",
    "results_df['Test MSE'] = pd.to_numeric(results_df['Test MSE'], errors='coerce')\n",
    "results_df['Test R²'] = pd.to_numeric(results_df['Test R²'], errors='coerce')\n",
    "\n",
    "# Option 1: Best by Test MSE (lowest error)\n",
    "best_mse_row = results_df.loc[results_df['Test MSE'].idxmin()]\n",
    "\n",
    "# Option 2: Best by Test R² (highest variance explained)\n",
    "best_r2_row = results_df.loc[results_df['Test R²'].idxmax()]\n",
    "\n",
    "# Create a new DataFrame that contains both \"best\" rows for clarity.\n",
    "best_results = pd.DataFrame([best_mse_row, best_r2_row])\n",
    "print(\"\\nSummary of Best Model Results:\")\n",
    "print(best_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total polynomial features: 14\n",
      "\n",
      "Option C (LassoCV directly on full features):\n",
      "Best alpha selected: 0.01778902890119987\n",
      "Test MSE: 0.19373047874210853\n",
      "Test R²: 0.522020815099516\n",
      "\n",
      "ElasticNetCV on full polynomial features:\n",
      "Best alpha: 0.016297508346206444\n",
      "Best l1_ratio: 0.9\n",
      "Test MSE: 0.1967981473574057\n",
      "Test R²: 0.5144521467422958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\souha\\anaconda3\\envs\\lab\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "500 fits failed out of a total of 1750.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "250 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\souha\\anaconda3\\envs\\lab\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\souha\\anaconda3\\envs\\lab\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\souha\\anaconda3\\envs\\lab\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\souha\\anaconda3\\envs\\lab\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'gamma' parameter of KernelRidge must be a float in the range [0.0, inf) or None. Got 'scale' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "250 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\souha\\anaconda3\\envs\\lab\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\souha\\anaconda3\\envs\\lab\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\souha\\anaconda3\\envs\\lab\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\souha\\anaconda3\\envs\\lab\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'gamma' parameter of KernelRidge must be a float in the range [0.0, inf) or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\souha\\anaconda3\\envs\\lab\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [-0.62814761 -2.66178382 -3.11285799 -1.55687146 -1.28915342         nan\n",
      "         nan -0.61166094 -2.36351972 -2.95135081 -1.55668233 -1.28910343\n",
      "         nan         nan -0.60014955 -2.09293337 -2.76943725 -1.55643183\n",
      " -1.28903721         nan         nan -0.59305788 -1.84912206 -2.57248261\n",
      " -1.55610015 -1.28894953         nan         nan -0.5900442  -1.63157966\n",
      " -2.36872175 -1.55566118 -1.28883347         nan         nan -0.59087393\n",
      " -1.44002536 -2.16834455 -1.55508053 -1.28867992         nan         nan\n",
      " -0.59531067 -1.27419597 -1.98178182 -1.55431302 -1.28847692         nan\n",
      "         nan -0.60303063 -1.13357251 -1.81766469 -1.55329949 -1.28820878\n",
      "         nan         nan -0.61360246 -1.01710919 -1.68118279 -1.55196277\n",
      " -1.28785501         nan         nan -0.62655996 -0.92308    -1.57342253\n",
      " -1.55020279 -1.28738897         nan         nan -0.64154546 -0.84912629\n",
      " -1.49180285 -1.54789066 -1.2867763          nan         nan -0.65845168\n",
      " -0.792497   -1.4312698  -1.54486208 -1.28597302         nan         nan\n",
      " -0.67748361 -0.75038274 -1.38572969 -1.54091039 -1.28492352         nan\n",
      "         nan -0.69910583 -0.72021426 -1.34929771 -1.53578041 -1.28355861\n",
      "         nan         nan -0.72389625 -0.69983598 -1.317147   -1.52916496\n",
      " -1.2817941          nan         nan -0.752353   -0.68754268 -1.28592352\n",
      " -1.5207073  -1.27953056         nan         nan -0.78469761 -0.68203023\n",
      " -1.25379764 -1.51001412 -1.27665557         nan         nan -0.82071413\n",
      " -0.68232922 -1.22026572 -1.49668507 -1.27304979         nan         nan\n",
      " -0.85966896 -0.68776416 -1.18581494 -1.48036446 -1.26859869         nan\n",
      "         nan -0.90035212 -0.6979371  -1.15154479 -1.4608181  -1.26321091\n",
      "         nan         nan -0.94124584 -0.7127029  -1.11881483 -1.43802914\n",
      " -1.2568429          nan         nan -0.98077495 -0.73210033 -1.08896734\n",
      " -1.41229369 -1.24952599         nan         nan -1.01755873 -0.75622316\n",
      " -1.06314998 -1.38428101 -1.24138834         nan         nan -1.05059031\n",
      " -0.78504293 -1.04223294 -1.35501713 -1.23266135         nan         nan\n",
      " -1.07930744 -0.81822174 -1.026786   -1.32576699 -1.22366215         nan\n",
      "         nan -1.10356325 -0.8549758  -1.01707249 -1.29783047 -1.21475141\n",
      "         nan         nan -1.12353313 -0.89405282 -1.01303665 -1.27231459\n",
      " -1.20627697         nan         nan -1.13959957 -0.93385426 -1.01429253\n",
      " -1.24996255 -1.1985212          nan         nan -1.15224614 -0.97267283\n",
      " -1.02014209 -1.23109364 -1.19166828         nan         nan -1.16197689\n",
      " -1.00896293 -1.02964339 -1.21565308 -1.18579775         nan         nan\n",
      " -1.16926448 -1.04155259 -1.04172394 -1.20332746 -1.18089978         nan\n",
      "         nan -1.1745235  -1.06974254 -1.05531006 -1.19367133 -1.17690164\n",
      "         nan         nan -1.1781026  -1.09329248 -1.06943643 -1.18620812\n",
      " -1.17369531         nan         nan -1.18028949 -1.11233242 -1.08331402\n",
      " -1.18049317 -1.17115993         nan         nan -1.18132353 -1.12724635\n",
      " -1.09635552 -1.17614365 -1.16917711         nan         nan -1.18141166\n",
      " -1.13856339 -1.1081708  -1.17284611 -1.16763966         nan         nan\n",
      " -1.18074367 -1.14687255 -1.11854588 -1.17035198 -1.16645537         nan\n",
      "         nan -1.17950284 -1.15276275 -1.12741299 -1.16846811 -1.16554773\n",
      "         nan         nan -1.17786926 -1.15678249 -1.1348159  -1.16704628\n",
      " -1.16485478         nan         nan -1.17601506 -1.15941398 -1.14087398\n",
      " -1.1659736  -1.16432729         nan         nan -1.1740943  -1.16105853\n",
      " -1.14574892 -1.16516451 -1.16392664         nan         nan -1.17223206\n",
      " -1.16203224 -1.14961781 -1.16455427 -1.16362285         nan         nan\n",
      " -1.17051757 -1.1625707  -1.15265401 -1.16409403 -1.16339279         nan\n",
      "         nan -1.16900345 -1.16284028 -1.15501547 -1.16374692 -1.16321874\n",
      "         nan         nan -1.16771058 -1.16295245 -1.15683921 -1.16348512\n",
      " -1.16308715         nan         nan -1.16663622 -1.1629783  -1.15823991\n",
      " -1.16328766 -1.16298773         nan         nan -1.16576261 -1.16296097\n",
      " -1.15931111 -1.16313873 -1.16291263         nan         nan -1.16506433\n",
      " -1.16292533 -1.16012764 -1.1630264  -1.16285594         nan         nan\n",
      " -1.16451366 -1.16288484 -1.16074848 -1.16294167 -1.16281314         nan\n",
      "         nan -1.16408392 -1.16284622 -1.16121962 -1.16287777 -1.16278084\n",
      "         nan         nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel Ridge Regression (RBF) with GridSearchCV:\n",
      "Best parameters: {'alpha': 0.0030888435964774815, 'gamma': 0.001}\n",
      "Test MSE: 0.20189700469113672\n",
      "Test R²: 0.501872052540676\n",
      "\n",
      "Stacking Regressor (meta-model = Ridge):\n",
      "Test MSE: 0.16011783853193742\n",
      "Test R²: 0.6049511958755727\n",
      "\n",
      "SVR (RBF kernel) standalone:\n",
      "Test MSE: 0.20975117345627092\n",
      "Test R²: 0.4824939494729231\n",
      "\n",
      "Weighted Averaging:\n",
      "Weights: [0.3438751  0.33851481 0.31761009]\n",
      "Test MSE: 0.1767275549555829\n",
      "Test R²: 0.5639710735471153\n",
      "\n",
      "Summary of Model Performance (Best Results):\n",
      "                          Option  Test MSE   Test R²\n",
      "3             Stacking Regressor  0.160118  0.604951\n",
      "5             Weighted Averaging  0.176728  0.563971\n",
      "0           C (LassoCV directly)  0.193730  0.522021\n",
      "1                   ElasticNetCV  0.196798  0.514452\n",
      "2  Kernel Ridge Regression (RBF)  0.201897  0.501872\n",
      "4           SVR (RBF standalone)  0.209751  0.482494\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Ridge, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ------------------------------\n",
    "# PREPROCESSING: Polynomial Feature Generation & Scaling\n",
    "# ------------------------------\n",
    "# Assume df_scaled is your preprocessed DataFrame with target column \"IE%\"\n",
    "X = df_scaled.drop('IE%', axis=1)\n",
    "y = df_scaled['IE%']\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Generate degree-2 polynomial features.\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# Scale the polynomial features.\n",
    "scaler = StandardScaler()\n",
    "X_train_poly_scaled = scaler.fit_transform(X_train_poly)\n",
    "X_test_poly_scaled = scaler.transform(X_test_poly)\n",
    "\n",
    "# Total number of polynomial features generated:\n",
    "n_poly_features = X_train_poly_scaled.shape[1]\n",
    "print(\"Total polynomial features:\", n_poly_features)\n",
    "\n",
    "# ------------------------------\n",
    "# Option C (from your previous code): LassoCV Directly on Full Polynomial Features\n",
    "# ------------------------------\n",
    "lasso_model = LassoCV(alphas=None, cv=5, random_state=42)\n",
    "lasso_model.fit(X_train_poly_scaled, y_train)\n",
    "y_pred_C = lasso_model.predict(X_test_poly_scaled)\n",
    "mse_C = mean_squared_error(y_test, y_pred_C)\n",
    "r2_C = r2_score(y_test, y_pred_C)\n",
    "print(\"\\nOption C (LassoCV directly on full features):\")\n",
    "print(\"Best alpha selected:\", lasso_model.alpha_)\n",
    "print(\"Test MSE:\", mse_C)\n",
    "print(\"Test R²:\", r2_C)\n",
    "\n",
    "# ------------------------------\n",
    "# 1. ElasticNetCV: Exploring l1_ratio parameter space\n",
    "# ------------------------------\n",
    "l1_ratios = [0.1, 0.3, 0.5, 0.7, 0.9]  # candidate values for the mixing parameter\n",
    "elastic_net_cv = ElasticNetCV(l1_ratio=l1_ratios, cv=5, random_state=42,\n",
    "                              alphas=np.logspace(-3, 3, 100))\n",
    "elastic_net_cv.fit(X_train_poly_scaled, y_train)\n",
    "y_pred_en = elastic_net_cv.predict(X_test_poly_scaled)\n",
    "mse_en = mean_squared_error(y_test, y_pred_en)\n",
    "r2_en = r2_score(y_test, y_pred_en)\n",
    "print(\"\\nElasticNetCV on full polynomial features:\")\n",
    "print(\"Best alpha:\", elastic_net_cv.alpha_)\n",
    "print(\"Best l1_ratio:\", elastic_net_cv.l1_ratio_)\n",
    "print(\"Test MSE:\", mse_en)\n",
    "print(\"Test R²:\", r2_en)\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Kernel Ridge Regression (KRR) with RBF kernel\n",
    "# ------------------------------\n",
    "param_grid_krr = {\n",
    "    'alpha': np.logspace(-3, 3, 50),\n",
    "    'gamma': [0.001, 0.01, 0.1, 1, 10, 'scale', 'auto']\n",
    "}\n",
    "grid_search_krr = GridSearchCV(KernelRidge(kernel='rbf'), param_grid_krr, cv=5,\n",
    "                               scoring='neg_mean_squared_error')\n",
    "grid_search_krr.fit(X_train_poly_scaled, y_train)\n",
    "best_krr = grid_search_krr.best_estimator_\n",
    "y_pred_krr = best_krr.predict(X_test_poly_scaled)\n",
    "mse_krr = mean_squared_error(y_test, y_pred_krr)\n",
    "r2_krr = r2_score(y_test, y_pred_krr)\n",
    "print(\"\\nKernel Ridge Regression (RBF) with GridSearchCV:\")\n",
    "print(\"Best parameters:\", grid_search_krr.best_params_)\n",
    "print(\"Test MSE:\", mse_krr)\n",
    "print(\"Test R²:\", r2_krr)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Stacking with a Meta‑Model\n",
    "# ------------------------------\n",
    "# Define base models:\n",
    "base_estimators = [\n",
    "    ('lasso', LassoCV(alphas=None, cv=5, random_state=42)),\n",
    "    ('elasticnet', ElasticNetCV(l1_ratio=l1_ratios, cv=5, random_state=42, alphas=np.logspace(-3, 3, 100))),\n",
    "    ('svr', SVR(kernel='rbf', C=10, gamma='scale'))  # using a preset parameter; you can refine via grid search\n",
    "]\n",
    "# Define a meta-model (using Ridge here)\n",
    "meta_model = Ridge(alpha=20.09)\n",
    "\n",
    "# Create the stacking regressor.\n",
    "stack_reg = StackingRegressor(estimators=base_estimators, final_estimator=meta_model, cv=5)\n",
    "stack_reg.fit(X_train_poly_scaled, y_train)\n",
    "y_pred_stack = stack_reg.predict(X_test_poly_scaled)\n",
    "mse_stack = mean_squared_error(y_test, y_pred_stack)\n",
    "r2_stack = r2_score(y_test, y_pred_stack)\n",
    "print(\"\\nStacking Regressor (meta-model = Ridge):\")\n",
    "print(\"Test MSE:\", mse_stack)\n",
    "print(\"Test R²:\", r2_stack)\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Weighted Averaging of Predictions\n",
    "# ------------------------------\n",
    "# We'll weight each model's prediction inversely proportional to its training MSE.\n",
    "# For demonstration, we use the predictions from Option C (LassoCV), ElasticNetCV, and SVR.\n",
    "# (For SVR, we use the best SVR from a previous GridSearchCV if available. Here, we'll use the SVR defined in stacking.)\n",
    "\n",
    "# If needed, run a separate grid search for SVR; here we reuse the SVR parameters from stacking.\n",
    "svr_model = SVR(kernel='rbf', C=10, gamma='scale')\n",
    "svr_model.fit(X_train_poly_scaled, y_train)\n",
    "y_pred_svr = svr_model.predict(X_test_poly_scaled)\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "print(\"\\nSVR (RBF kernel) standalone:\")\n",
    "print(\"Test MSE:\", mse_svr)\n",
    "print(\"Test R²:\", r2_svr)\n",
    "\n",
    "# Collect the MSE values from three models.\n",
    "mse_values = np.array([mse_C, mse_en, mse_svr])\n",
    "# Compute weights as the inverse of MSE (lower MSE gets higher weight).\n",
    "weights = 1.0 / mse_values\n",
    "weights /= weights.sum()  # Normalize to sum to 1\n",
    "print(\"\\nWeighted Averaging:\")\n",
    "print(\"Weights:\", weights)\n",
    "\n",
    "# Weighted ensemble prediction:\n",
    "y_pred_weighted = weights[0]*y_pred_C + weights[1]*y_pred_en + weights[2]*y_pred_svr\n",
    "mse_weighted = mean_squared_error(y_test, y_pred_weighted)\n",
    "r2_weighted = r2_score(y_test, y_pred_weighted)\n",
    "print(\"Test MSE:\", mse_weighted)\n",
    "print(\"Test R²:\", r2_weighted)\n",
    "\n",
    "# ------------------------------\n",
    "# Summary: Collect results in a table.\n",
    "# ------------------------------\n",
    "results = [\n",
    "    {'Option': 'C (LassoCV directly)', 'Test MSE': mse_C, 'Test R²': r2_C},\n",
    "    {'Option': 'ElasticNetCV', 'Test MSE': mse_en, 'Test R²': r2_en},\n",
    "    {'Option': 'Kernel Ridge Regression (RBF)', 'Test MSE': mse_krr, 'Test R²': r2_krr},\n",
    "    {'Option': 'Stacking Regressor', 'Test MSE': mse_stack, 'Test R²': r2_stack},\n",
    "    {'Option': 'SVR (RBF standalone)', 'Test MSE': mse_svr, 'Test R²': r2_svr},\n",
    "    {'Option': 'Weighted Averaging', 'Test MSE': mse_weighted, 'Test R²': r2_weighted}\n",
    "]\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSummary of Model Performance (Best Results):\")\n",
    "print(results_df.sort_values(by='Test MSE'))  # Sorting by Test MSE (lowest is best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total polynomial features: 14\n",
      "Option A: LassoCV selected 14 out of 14 features.\n",
      "Option B: Full polynomial feature set has 14 features. Selecting 7 features via RFE.\n",
      "RFE selected 7 features.\n",
      "Option D: RFE with LassoCV as base estimator selected 7 features.\n",
      "\n",
      "Stacking Regressor (meta-model = LassoCV):\n",
      "Test MSE: 0.16652075638504785\n",
      "Test R²: 0.5891536741005478\n",
      "Meta Best Alpha: 0.03252765968319807\n",
      "\n",
      "Weighted Averaging Weights: [0.3671947  0.36147091 0.27133439]\n",
      "\n",
      "Summary of Model Performance (All Options):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Option</th>\n",
       "      <th>Test MSE</th>\n",
       "      <th>Test R²</th>\n",
       "      <th>Best Alpha</th>\n",
       "      <th>Best Params</th>\n",
       "      <th>Meta Best Alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Stacking Regressor (meta = Ridge)</td>\n",
       "      <td>0.160729</td>\n",
       "      <td>0.603444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Stacking Regressor (meta = LassoCV)</td>\n",
       "      <td>0.166521</td>\n",
       "      <td>0.589154</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.032528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C (LassoCV directly)</td>\n",
       "      <td>0.193730</td>\n",
       "      <td>0.522021</td>\n",
       "      <td>0.017789</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ElasticNetCV</td>\n",
       "      <td>0.196798</td>\n",
       "      <td>0.514452</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kernel Ridge Regression (RBF)</td>\n",
       "      <td>0.201897</td>\n",
       "      <td>0.501872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'alpha': 0.0030888435964774815, 'gamma': 0.001}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Weighted Averaging</td>\n",
       "      <td>0.207678</td>\n",
       "      <td>0.487609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D (Ridge on RFE(LassoCV)-selected)</td>\n",
       "      <td>0.232327</td>\n",
       "      <td>0.426794</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B (Ridge on RFE-selected)</td>\n",
       "      <td>0.232538</td>\n",
       "      <td>0.426274</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A (Ridge on LassoCV-selected)</td>\n",
       "      <td>0.249418</td>\n",
       "      <td>0.384625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SVR (RBF standalone)</td>\n",
       "      <td>0.262174</td>\n",
       "      <td>0.353155</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'C': 100, 'gamma': 0.001}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Nested CV (Ridge Pipeline)</td>\n",
       "      <td>0.545164</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Option  Test MSE   Test R²  Best Alpha  \\\n",
       "7     Stacking Regressor (meta = Ridge)  0.160729  0.603444         NaN   \n",
       "8   Stacking Regressor (meta = LassoCV)  0.166521  0.589154         NaN   \n",
       "2                  C (LassoCV directly)  0.193730  0.522021    0.017789   \n",
       "4                          ElasticNetCV  0.196798  0.514452         NaN   \n",
       "5         Kernel Ridge Regression (RBF)  0.201897  0.501872         NaN   \n",
       "9                    Weighted Averaging  0.207678  0.487609         NaN   \n",
       "3    D (Ridge on RFE(LassoCV)-selected)  0.232327  0.426794         NaN   \n",
       "1             B (Ridge on RFE-selected)  0.232538  0.426274         NaN   \n",
       "0         A (Ridge on LassoCV-selected)  0.249418  0.384625         NaN   \n",
       "6                  SVR (RBF standalone)  0.262174  0.353155         NaN   \n",
       "10           Nested CV (Ridge Pipeline)  0.545164       NaN         NaN   \n",
       "\n",
       "                                         Best Params  Meta Best Alpha  \n",
       "7                                                NaN              NaN  \n",
       "8                                                NaN         0.032528  \n",
       "2                                                NaN              NaN  \n",
       "4                                                NaN              NaN  \n",
       "5   {'alpha': 0.0030888435964774815, 'gamma': 0.001}              NaN  \n",
       "9                                                NaN              NaN  \n",
       "3                                                NaN              NaN  \n",
       "1                                                NaN              NaN  \n",
       "0                                                NaN              NaN  \n",
       "6                         {'C': 100, 'gamma': 0.001}              NaN  \n",
       "10                                               NaN              NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Ridge, LassoCV, ElasticNetCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ================================\n",
    "# Helper function to evaluate a model.\n",
    "# It accepts an extra_info parameter.\n",
    "# ================================\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, option_name, extra_info=None):\n",
    "    if extra_info is None:\n",
    "        extra_info = {}\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    result = {'Option': option_name, 'Test MSE': mse, 'Test R²': r2}\n",
    "    result.update(extra_info)\n",
    "    return result, y_pred\n",
    "\n",
    "# ================================\n",
    "# PREPROCESSING: Polynomial Feature Generation & Scaling\n",
    "# ================================\n",
    "# Assume df_scaled is your preprocessed DataFrame with target column \"IE%\"\n",
    "X = df_scaled.drop('IE%', axis=1)\n",
    "y = df_scaled['IE%']\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Generate degree-2 polynomial features.\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# Scale the polynomial features.\n",
    "scaler = StandardScaler()\n",
    "X_train_poly_scaled = scaler.fit_transform(X_train_poly)\n",
    "X_test_poly_scaled = scaler.transform(X_test_poly)\n",
    "\n",
    "n_poly_features = X_train_poly_scaled.shape[1]\n",
    "print(\"Total polynomial features:\", n_poly_features)\n",
    "\n",
    "# ================================\n",
    "# Container for results and predictions\n",
    "# ================================\n",
    "results_list = {}\n",
    "predictions = {}\n",
    "\n",
    "# ------------------------------\n",
    "# Option A: LassoCV for feature selection, then Ridge on selected features\n",
    "# ------------------------------\n",
    "lasso_cv = LassoCV(alphas=None, cv=5, random_state=42, max_iter=10000)\n",
    "lasso_cv.fit(X_train_poly_scaled, y_train)\n",
    "nonzero_indices = np.where(lasso_cv.coef_ != 0)[0]\n",
    "n_nonzero_A = len(nonzero_indices)\n",
    "print(f\"Option A: LassoCV selected {n_nonzero_A} out of {n_poly_features} features.\")\n",
    "\n",
    "X_train_A = X_train_poly_scaled[:, nonzero_indices]\n",
    "X_test_A  = X_test_poly_scaled[:, nonzero_indices]\n",
    "\n",
    "ridge_A = Ridge(alpha=20.09)\n",
    "result_A, y_pred_A = evaluate_model(ridge_A, X_train_A, y_train, X_test_A, y_test,\n",
    "                                    'A (Ridge on LassoCV-selected)')\n",
    "results_list['A (Ridge on LassoCV-selected)'] = result_A\n",
    "predictions['A'] = y_pred_A\n",
    "\n",
    "# ------------------------------\n",
    "# Option B: RFE with Ridge estimator, then Ridge on RFE-selected features\n",
    "# ------------------------------\n",
    "ridge_estimator = Ridge(alpha=20.09)\n",
    "n_features_to_select_B = n_poly_features // 2\n",
    "print(f\"Option B: Full polynomial feature set has {n_poly_features} features. Selecting {n_features_to_select_B} features via RFE.\")\n",
    "\n",
    "rfe = RFE(estimator=ridge_estimator, n_features_to_select=n_features_to_select_B, step=1)\n",
    "rfe.fit(X_train_poly_scaled, y_train)\n",
    "selected_features_B = rfe.support_\n",
    "n_selected_B = np.sum(selected_features_B)\n",
    "print(f\"RFE selected {n_selected_B} features.\")\n",
    "\n",
    "X_train_B = X_train_poly_scaled[:, selected_features_B]\n",
    "X_test_B  = X_test_poly_scaled[:, selected_features_B]\n",
    "\n",
    "ridge_B = Ridge(alpha=20.09)\n",
    "result_B, y_pred_B = evaluate_model(ridge_B, X_train_B, y_train, X_test_B, y_test,\n",
    "                                    'B (Ridge on RFE-selected)')\n",
    "results_list['B (Ridge on RFE-selected)'] = result_B\n",
    "predictions['B'] = y_pred_B\n",
    "\n",
    "# ------------------------------\n",
    "# Option C: Train LassoCV directly on full polynomial features\n",
    "# ------------------------------\n",
    "lasso_model = LassoCV(alphas=None, cv=5, random_state=42, max_iter=10000)\n",
    "result_C, y_pred_C = evaluate_model(lasso_model, X_train_poly_scaled, y_train, \n",
    "                                    X_test_poly_scaled, y_test,\n",
    "                                    'C (LassoCV directly)')\n",
    "result_C['Best Alpha'] = lasso_model.alpha_\n",
    "results_list['C (LassoCV directly)'] = result_C\n",
    "predictions['C'] = y_pred_C\n",
    "\n",
    "# ------------------------------\n",
    "# Option D: RFE with LassoCV as base estimator, then Ridge on selected features\n",
    "# ------------------------------\n",
    "lasso_cv_for_rfe = LassoCV(alphas=None, cv=5, random_state=42, max_iter=10000)\n",
    "n_features_to_select_D = n_poly_features // 2\n",
    "rfe_D = RFE(estimator=lasso_cv_for_rfe, n_features_to_select=n_features_to_select_D, step=1)\n",
    "rfe_D.fit(X_train_poly_scaled, y_train)\n",
    "selected_features_D = rfe_D.support_\n",
    "n_selected_D = np.sum(selected_features_D)\n",
    "print(f\"Option D: RFE with LassoCV as base estimator selected {n_selected_D} features.\")\n",
    "\n",
    "X_train_D = X_train_poly_scaled[:, selected_features_D]\n",
    "X_test_D  = X_test_poly_scaled[:, selected_features_D]\n",
    "\n",
    "ridge_D = Ridge(alpha=20.09)\n",
    "result_D, y_pred_D = evaluate_model(ridge_D, X_train_D, y_train, X_test_D, y_test,\n",
    "                                    'D (Ridge on RFE(LassoCV)-selected)')\n",
    "results_list['D (Ridge on RFE(LassoCV)-selected)'] = result_D\n",
    "predictions['D'] = y_pred_D\n",
    "\n",
    "# ------------------------------\n",
    "# Option E: ElasticNetCV on full polynomial features\n",
    "# ------------------------------\n",
    "l1_ratios = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "elastic_net_cv = ElasticNetCV(l1_ratio=l1_ratios, cv=5, random_state=42,\n",
    "                              alphas=np.logspace(-3, 3, 100), max_iter=10000)\n",
    "result_en, y_pred_en = evaluate_model(elastic_net_cv, X_train_poly_scaled, y_train,\n",
    "                                      X_test_poly_scaled, y_test, 'ElasticNetCV')\n",
    "results_list['ElasticNetCV'] = result_en\n",
    "predictions['ElasticNetCV'] = y_pred_en\n",
    "\n",
    "# ------------------------------\n",
    "# Option F: Kernel Ridge Regression (KRR) with RBF kernel using GridSearchCV\n",
    "# ------------------------------\n",
    "param_grid_krr = {\n",
    "    'alpha': np.logspace(-3, 3, 50),\n",
    "    'gamma': [0.001, 0.01, 0.1, 1, 10]\n",
    "}\n",
    "grid_search_krr = GridSearchCV(KernelRidge(kernel='rbf'), param_grid_krr, cv=5,\n",
    "                               scoring='neg_mean_squared_error')\n",
    "grid_search_krr.fit(X_train_poly_scaled, y_train)\n",
    "best_krr = grid_search_krr.best_estimator_\n",
    "y_pred_krr = best_krr.predict(X_test_poly_scaled)\n",
    "mse_krr = mean_squared_error(y_test, y_pred_krr)\n",
    "r2_krr = r2_score(y_test, y_pred_krr)\n",
    "result_krr = {\n",
    "    'Option': 'Kernel Ridge Regression (RBF)',\n",
    "    'Test MSE': mse_krr,\n",
    "    'Test R²': r2_krr,\n",
    "    'Best Params': grid_search_krr.best_params_\n",
    "}\n",
    "results_list['Kernel Ridge Regression (RBF)'] = result_krr\n",
    "\n",
    "# ------------------------------\n",
    "# Option G: SVR (RBF standalone) using GridSearchCV\n",
    "# ------------------------------\n",
    "param_grid_svr = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1, 10]\n",
    "}\n",
    "grid_search_svr = GridSearchCV(SVR(kernel='rbf'), param_grid_svr, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_svr.fit(X_train_poly_scaled, y_train)\n",
    "best_svr = grid_search_svr.best_estimator_\n",
    "result_svr, y_pred_svr = evaluate_model(best_svr, X_train_poly_scaled, y_train,\n",
    "                                        X_test_poly_scaled, y_test,\n",
    "                                        'SVR (RBF standalone)',\n",
    "                                        extra_info={'Best Params': grid_search_svr.best_params_})\n",
    "results_list['SVR (RBF standalone)'] = result_svr\n",
    "predictions['SVR'] = y_pred_svr\n",
    "\n",
    "# ------------------------------\n",
    "# Option H: Stacking Regressor with Meta-Model (Ridge as meta-model)\n",
    "# ------------------------------\n",
    "base_estimators = [\n",
    "    ('lasso', LassoCV(alphas=None, cv=5, random_state=42, max_iter=10000)),\n",
    "    ('elasticnet', ElasticNetCV(l1_ratio=l1_ratios, cv=5, random_state=42,\n",
    "                                 alphas=np.logspace(-3, 3, 100), max_iter=10000)),\n",
    "    ('svr', SVR(kernel='rbf', C=10, gamma=0.01))\n",
    "]\n",
    "meta_model_ridge = Ridge(alpha=20.09)\n",
    "stack_reg_ridge = StackingRegressor(estimators=base_estimators, final_estimator=meta_model_ridge, cv=5)\n",
    "result_stack_ridge, y_pred_stack_ridge = evaluate_model(stack_reg_ridge, X_train_poly_scaled, y_train,\n",
    "                                                        X_test_poly_scaled, y_test,\n",
    "                                                        'Stacking Regressor (meta = Ridge)')\n",
    "results_list['Stacking Regressor (meta = Ridge)'] = result_stack_ridge\n",
    "predictions['Stacking_Ridge'] = y_pred_stack_ridge\n",
    "\n",
    "# ------------------------------\n",
    "# Option I: Stacking Regressor with Meta-Model (LassoCV as meta-model)\n",
    "# ------------------------------\n",
    "base_estimators = [\n",
    "    ('lasso', LassoCV(alphas=None, cv=5, random_state=42, max_iter=10000)),\n",
    "    ('elasticnet', ElasticNetCV(l1_ratio=l1_ratios, cv=5, random_state=42,\n",
    "                                 alphas=np.logspace(-3, 3, 100), max_iter=10000)),\n",
    "    ('svr', SVR(kernel='rbf', C=10, gamma=0.01))\n",
    "]\n",
    "meta_model_lasso = LassoCV(alphas=None, cv=5, random_state=42, max_iter=10000)\n",
    "stack_reg_lasso = StackingRegressor(estimators=base_estimators, final_estimator=meta_model_lasso, cv=5)\n",
    "result_stack_lasso, y_pred_stack_lasso = evaluate_model(stack_reg_lasso, X_train_poly_scaled, y_train,\n",
    "                                                          X_test_poly_scaled, y_test,\n",
    "                                                          'Stacking Regressor (meta = LassoCV)')\n",
    "# Access the fitted meta-model via final_estimator_\n",
    "result_stack_lasso['Meta Best Alpha'] = stack_reg_lasso.final_estimator_.alpha_\n",
    "results_list['Stacking Regressor (meta = LassoCV)'] = result_stack_lasso\n",
    "predictions['Stacking_LassoMeta'] = y_pred_stack_lasso\n",
    "\n",
    "print(\"\\nStacking Regressor (meta-model = LassoCV):\")\n",
    "print(\"Test MSE:\", result_stack_lasso['Test MSE'])\n",
    "print(\"Test R²:\", result_stack_lasso['Test R²'])\n",
    "print(\"Meta Best Alpha:\", stack_reg_lasso.final_estimator_.alpha_)\n",
    "\n",
    "# ------------------------------\n",
    "# Option J: Weighted Averaging of Predictions from Option C, ElasticNetCV, and SVR\n",
    "# ------------------------------\n",
    "mse_vals = np.array([\n",
    "    results_list['C (LassoCV directly)']['Test MSE'],\n",
    "    results_list['ElasticNetCV']['Test MSE'],\n",
    "    results_list['SVR (RBF standalone)']['Test MSE']\n",
    "])\n",
    "weights = 1.0 / mse_vals\n",
    "weights /= weights.sum()\n",
    "print(\"\\nWeighted Averaging Weights:\", weights)\n",
    "y_pred_weighted = weights[0]*y_pred_C + weights[1]*y_pred_en + weights[2]*y_pred_svr\n",
    "mse_weighted = mean_squared_error(y_test, y_pred_weighted)\n",
    "r2_weighted = r2_score(y_test, y_pred_weighted)\n",
    "result_weighted = {\n",
    "    'Option': 'Weighted Averaging',\n",
    "    'Test MSE': mse_weighted,\n",
    "    'Test R²': r2_weighted\n",
    "}\n",
    "results_list['Weighted Averaging'] = result_weighted\n",
    "\n",
    "# ------------------------------\n",
    "# Option K: Nested Cross-Validation Example with Ridge Regression Pipeline\n",
    "# ------------------------------\n",
    "pipeline_ridge = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ridgecv', Ridge(alpha=20.09))  # Here we use Ridge (or RidgeCV) in the pipeline\n",
    "])\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "nested_scores = cross_val_score(pipeline_ridge, X, y, cv=outer_cv, scoring='neg_mean_squared_error')\n",
    "avg_nested_mse = -np.mean(nested_scores)\n",
    "result_nested = {\n",
    "    'Option': 'Nested CV (Ridge Pipeline)',\n",
    "    'Test MSE': avg_nested_mse,\n",
    "    'Test R²': np.nan\n",
    "}\n",
    "results_list['Nested CV (Ridge Pipeline)'] = result_nested\n",
    "\n",
    "# ------------------------------\n",
    "# Create a summary DataFrame from all results.\n",
    "# ------------------------------\n",
    "results_df = pd.DataFrame(list(results_list.values()))\n",
    "results_df['Test MSE'] = pd.to_numeric(results_df['Test MSE'], errors='coerce')\n",
    "results_df['Test R²'] = pd.to_numeric(results_df['Test R²'], errors='coerce')\n",
    "results_df = results_df.sort_values(by='Test MSE')\n",
    "\n",
    "print(\"\\nSummary of Model Performance (All Options):\")\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
